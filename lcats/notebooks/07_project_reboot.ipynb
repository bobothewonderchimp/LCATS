{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reboot of LCATS Story Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third-party modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add imports from within the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to the path so we can import modules from the parent directory.\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from lcats import stories\n",
    "from lcats import utils\n",
    "from lcats.datasets import torchdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the following code is run from lcats/notebooks in VSCode and the data is in lcats/data ...\n",
    "CURRENT_PATH = os.path.abspath(os.curdir)  # This is where the notebook is executing.\n",
    "PROJECT_ROOT = os.path.dirname(CURRENT_PATH)   # This should be the root of the project.\n",
    "DEV_CORPUS = os.path.abspath(os.path.join(PROJECT_ROOT, 'data'))  # Local copy of the data.\n",
    "GIT_CORPUS = os.path.abspath(os.path.join(PROJECT_ROOT, '../corpora'))  # Data in the git repo.\n",
    "OPENIA_API_KEYS_ENV = os.path.abspath(os.path.join(PROJECT_ROOT, '../.secrets/openai_api_keys.env'))  # Local OpenAI API key.\n",
    "\n",
    "def check_path(path, description):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Found {description} at: {path}\")\n",
    "    else:\n",
    "        print(f\"Missing {description} from: {path}\")\n",
    "\n",
    "check_path(DEV_CORPUS, \"DEV_CORPUS\")\n",
    "check_path(GIT_CORPUS, \"GIT_CORPUS\")\n",
    "check_path(OPENIA_API_KEYS_ENV, \"OPENIA_API_KEYS_ENV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(OPENIA_API_KEYS_ENV)\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that we can get a client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "print(f\"Loaded OpenAI client: {client} with version: {client._version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the API is working. This week. And that you have credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Write a one-sentence bedtime story about a starship captain visiting a planet.\"\n",
    ")\n",
    "\n",
    "utils.pprint(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Story Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(stories)\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If run from within a notebook, the corpora root is two paths up from the notebook's location.\n",
    "CORPORA_ROOT = GIT_CORPUS  # Checked-in corpora\n",
    "# CORPORA_ROOT = DEV_CORPUS  # Command line working corpora\n",
    "\n",
    "# Now load the corpora\n",
    "corpora = stories.Corpora(CORPORA_ROOT)\n",
    "\n",
    "print(\"Loaded corpora:\")\n",
    "print(f\" - root: {corpora.corpora_root}\")\n",
    "print(f\" - corpora: {len(corpora.corpora)}\")\n",
    "print(f\" - stories: {len(corpora.stories)}\")\n",
    "print()\n",
    "print(f\"Example story: corpora.stories[0]:\")\n",
    "print(corpora.stories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_story = corpora.stories[0]\n",
    "print(len(example_story.body))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scene and Sequel Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code suggested by ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENE_SEQUEL_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that breaks down stories into structured events.\n",
    "Each event is labeled as \"scene\", \"sequel\", or \"none\" (if it doesn't fit exactly).\n",
    "Follow these definitions:\n",
    "\n",
    "- scene: a segment where a character with a goal attempts to achieve it, leading to success or disaster.\n",
    "- sequel: a segment after a disaster or success, where a character reacts, processes emotions, considers options, and forms a new goal.\n",
    "\n",
    "Your output MUST be valid JSON and only the JSON without any other text or comments.\n",
    "\"\"\"\n",
    "\n",
    "SCENE_SEQUEL_USER_PROMPT_TEMPLATE = \"\"\"\n",
    "I will give you a story in plain text.\n",
    "1. Read the story carefully.\n",
    "2. Identify major events or paragraphs that qualify as scenes or sequels (or 'none' if it doesn't clearly fit).\n",
    "3. For each event, provide:\n",
    "   - event_text: the text snippet or summary\n",
    "   - event_type: 'scene' or 'sequel' or 'none'\n",
    "   - reason: a short explanation of why you classified it that way\n",
    "4. Return a JSON dictionary with one key named \"events\" - the output must be valid JSON and only the JSON.\n",
    "Your output MUST be valid JSON and only the JSON without any other text or comments.\n",
    "\n",
    "STORY:\n",
    "\\\"\\\"\\\"{story_text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "def build_scene_sequel_prompt(story_text: str) -> list:\n",
    "    \"\"\"\n",
    "    Build the chat messages for the OpenAI ChatCompletion call.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SCENE_SEQUEL_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": SCENE_SEQUEL_USER_PROMPT_TEMPLATE.format(story_text=story_text)}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example_story.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = build_scene_sequel_prompt(example_story.body)\n",
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "def extract_scenes_and_sequels(story_text: str, model_name=\"gpt-3.5-turbo\"):\n",
    "    messages = build_scene_sequel_prompt(story_text)\n",
    "    \n",
    "    # Provide your API key, then:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.2,  # slightly creative but mostly deterministic\n",
    "    )\n",
    "\n",
    "    \n",
    "    # The assistant response is in response.choices[0].message[\"content\"]\n",
    "    raw_output = response.choices[0].message.content\n",
    "    \n",
    "    # Attempt to parse the JSON\n",
    "    try:\n",
    "        parsed_output = utils.extract_json(raw_output)\n",
    "        parsing_error = None\n",
    "\n",
    "    except json.JSONDecodeError as exc:\n",
    "        # The LLM might have returned invalid JSON or additional text around JSON\n",
    "        # In that case, you can attempt to strip out the JSON portion or re-prompt\n",
    "        parsed_output = None\n",
    "        parsing_error = str(exc)\n",
    "\n",
    "    # Expecting something like: { \"events\": [ ... ] }\n",
    "    if isinstance(parsed_output, dict) and \"events\" in parsed_output:\n",
    "        extracted_output = parsed_output[\"events\"]\n",
    "        extraction_error = None\n",
    "    else:\n",
    "        # If we didn't get the expected structure, handle fallback\n",
    "        extracted_output = None\n",
    "        extraction_error = \"Expected 'events' key in JSON response.\"\n",
    "\n",
    "    return {\n",
    "        \"story_text\": story_text,\n",
    "        \"model_name\": model_name,\n",
    "        \"messages\": messages,\n",
    "        \"response\": response,\n",
    "        \"raw_output\": raw_output,\n",
    "        \"parsed_output\": parsed_output,\n",
    "        \"extracted_output\": extracted_output,\n",
    "        \"parsing_error\": parsing_error,\n",
    "        \"extraction_error\": extraction_error,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gpt_35_turbo = extract_scenes_and_sequels(example_story.body)\n",
    "\n",
    "len(result_gpt_35_turbo[\"extracted_output\"]), result_gpt_35_turbo[\"extracted_output\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this cell takes 30-90 seconds and is an expensive GPT 4.0 call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gpt_4o = extract_scenes_and_sequels(\n",
    "    corpora.stories[0].body, model_name=\"gpt-4o\")\n",
    "\n",
    "len(result_gpt_4o[\"extracted_output\"]), result_gpt_4o[\"extracted_output\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LCATS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
