{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reboot of LCATS Story Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third-party modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add imports from within the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to the path so we can import modules from the parent directory.\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from lcats import stories\n",
    "from lcats import utils\n",
    "from lcats.datasets import torchdata\n",
    "from lcats.gatherers import extractors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the following code is run from lcats/notebooks in VSCode and the data is in lcats/data ...\n",
    "CURRENT_PATH = os.path.abspath(os.curdir)  # This is where the notebook is executing.\n",
    "PROJECT_ROOT = os.path.dirname(CURRENT_PATH)   # This should be the root of the project.\n",
    "DEV_CORPUS = os.path.abspath(os.path.join(PROJECT_ROOT, 'data'))  # Local copy of the data.\n",
    "DEV_OUTPUT = os.path.abspath(os.path.join(PROJECT_ROOT, 'output'))  # Local copy of the data.\n",
    "GIT_CORPUS = os.path.abspath(os.path.join(PROJECT_ROOT, '../corpora'))  # Data in the git repo.\n",
    "OPENIA_API_KEYS_ENV = os.path.abspath(os.path.join(PROJECT_ROOT, '../.secrets/openai_api_keys.env'))  # Local OpenAI API key.\n",
    "\n",
    "def check_path(path, description):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Found {description} at: {path}\")\n",
    "    else:\n",
    "        print(f\"Missing {description} from: {path}\")\n",
    "\n",
    "check_path(DEV_CORPUS, \"DEV_CORPUS\")\n",
    "check_path(DEV_OUTPUT, \"DEV_OUTPUT\")\n",
    "check_path(GIT_CORPUS, \"GIT_CORPUS\")\n",
    "check_path(OPENIA_API_KEYS_ENV, \"OPENIA_API_KEYS_ENV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(OPENIA_API_KEYS_ENV)\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that we can get a client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "print(f\"Loaded OpenAI client: {client} with version: {client._version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the API is working. This week. And that you have credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Write a one-sentence bedtime story about a starship captain visiting a planet.\"\n",
    ")\n",
    "\n",
    "print(f\"Story generated on: {date.today()}:\")\n",
    "utils.pprint(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Story Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the modules to ensure we have the latest code, if doing active development.\n",
    "if False: \n",
    "    from importlib import reload\n",
    "    reload(stories)\n",
    "    reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If run from within a notebook, the corpora root is two paths up from the notebook's location.\n",
    "CORPORA_ROOT = GIT_CORPUS  # Checked-in corpora\n",
    "# CORPORA_ROOT = DEV_CORPUS  # Command line working corpora\n",
    "\n",
    "# Now load the corpora\n",
    "corpora = stories.Corpora(CORPORA_ROOT)\n",
    "\n",
    "print(\"Loaded corpora:\")\n",
    "print(f\" - root: {corpora.corpora_root}\")\n",
    "print(f\" - corpora: {len(corpora.corpora)}\")\n",
    "print(f\" - stories: {len(corpora.stories)}\")\n",
    "print()\n",
    "print(f\"Example story: corpora.stories[0]:\")\n",
    "print(corpora.stories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_story = corpora.stories[0]\n",
    "print(f\"Story type: {type(example_story)} with a body of {len(example_story.body)} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scene and Sequel Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code suggested by ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENE_SEQUEL_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that breaks down stories into structured events.\n",
    "Each event is labeled as \"scene\", \"sequel\", or \"none\" (if it doesn't fit exactly).\n",
    "Follow these definitions:\n",
    "\n",
    "- scene: a segment where a character with a goal attempts to achieve it, leading to success or disaster.\n",
    "- sequel: a segment after a disaster or success, where a character reacts, processes emotions, considers options, and forms a new goal.\n",
    "\n",
    "Your output MUST be valid JSON and only the JSON without any other text or comments.\n",
    "\"\"\"\n",
    "\n",
    "SCENE_SEQUEL_USER_PROMPT_TEMPLATE = \"\"\"\n",
    "I will give you a story in plain text.\n",
    "1. Read the story carefully.\n",
    "2. Identify major events or paragraphs that qualify as scenes or sequels (or 'none' if it doesn't clearly fit).\n",
    "3. For each event, provide:\n",
    "   - event_text: the text snippet or summary\n",
    "   - event_type: 'scene' or 'sequel' or 'none'\n",
    "   - reason: a short explanation of why you classified it that way\n",
    "4. Return a JSON dictionary with one key named \"events\" - the output must be valid JSON and only the JSON.\n",
    "Your output MUST be valid JSON and only the JSON without any other text or comments.\n",
    "\n",
    "STORY:\n",
    "\\\"\\\"\\\"{story_text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "def build_scene_sequel_prompt(story_text: str) -> list:\n",
    "    \"\"\"\n",
    "    Build the chat messages for the OpenAI ChatCompletion call.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SCENE_SEQUEL_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": SCENE_SEQUEL_USER_PROMPT_TEMPLATE.format(story_text=story_text)}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = build_scene_sequel_prompt(example_story.body)\n",
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scenes_and_sequels(story_text: str, model_name=\"gpt-3.5-turbo\"):\n",
    "    messages = build_scene_sequel_prompt(story_text)\n",
    "    \n",
    "    # Provide your API key, then:\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.2,  # slightly creative but mostly deterministic\n",
    "    )\n",
    "\n",
    "    \n",
    "    # The assistant response is in response.choices[0].message[\"content\"]\n",
    "    raw_output = response.choices[0].message.content\n",
    "    \n",
    "    # Attempt to parse the JSON\n",
    "    try:\n",
    "        parsed_output = utils.extract_json(raw_output)\n",
    "        parsing_error = None\n",
    "\n",
    "    except json.JSONDecodeError as exc:\n",
    "        # The LLM might have returned invalid JSON or additional text around JSON\n",
    "        # In that case, you can attempt to strip out the JSON portion or re-prompt\n",
    "        parsed_output = None\n",
    "        parsing_error = str(exc)\n",
    "\n",
    "    # Expecting something like: { \"events\": [ ... ] }\n",
    "    if isinstance(parsed_output, dict) and \"events\" in parsed_output:\n",
    "        extracted_output = parsed_output[\"events\"]\n",
    "        extraction_error = None\n",
    "    else:\n",
    "        # If we didn't get the expected structure, handle fallback\n",
    "        extracted_output = None\n",
    "        extraction_error = \"Expected 'events' key in JSON response.\"\n",
    "\n",
    "    return {\n",
    "        \"story_text\": story_text,\n",
    "        \"model_name\": model_name,\n",
    "        \"messages\": messages,\n",
    "        \"response\": response,\n",
    "        \"raw_output\": raw_output,\n",
    "        \"parsed_output\": parsed_output,\n",
    "        \"extracted_output\": extracted_output,\n",
    "        \"parsing_error\": parsing_error,\n",
    "        \"extraction_error\": extraction_error,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gpt_35_turbo = extract_scenes_and_sequels(example_story.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_gpt_35_turbo[\"extracted_output\"]), result_gpt_35_turbo[\"extracted_output\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this cell takes 30-90 seconds and is an expensive GPT 4.0 call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gpt_4o = extract_scenes_and_sequels(\n",
    "    corpora.stories[0].body, model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_gpt_4o[\"extracted_output\"]), result_gpt_4o[\"extracted_output\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(corpora.stories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_serializable(result, nonserializable_key=\"response\"):\n",
    "    \"\"\"\n",
    "    Remove a non-serializable key from the result dictionary.\n",
    "\n",
    "    Args:\n",
    "        result (dict): The dictionary to clean.\n",
    "        nonserializable_key (str): The key to remove if present.\n",
    "\n",
    "    Returns:\n",
    "        dict: A shallow copy of the dictionary with the specified key removed.\n",
    "    \"\"\"\n",
    "    result = dict(result)  # shallow copy to avoid mutating original\n",
    "    result.pop(nonserializable_key, None)\n",
    "    return result\n",
    "\n",
    "def extract_all_and_write(corpora, extractor, model_name, output_dir, file_namer, serializer):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create a subdirectory for the model\n",
    "    model_dir = os.path.join(output_dir, \"scene_extraction\", model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    for story in corpora.stories:\n",
    "        filename = file_namer(story.name) + \"-scenes.json\"\n",
    "        filepath = os.path.join(model_dir, filename)\n",
    "\n",
    "        # Skip already-processed files\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"Skipping already processed story: {story.name}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"Processing story: {story.name}\")\n",
    "            result = extractor(story.body, model_name=model_name)\n",
    "            serialized_result = serializer(result)\n",
    "            with open(filepath, \"w\") as f:\n",
    "                json.dump(serialized_result, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {story.name}: {e}\")\n",
    "\n",
    "    print(\"Scene extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_all_and_write(\n",
    "    corpora,\n",
    "    extract_scenes_and_sequels,\n",
    "    model_name=\"gpt-4o\",\n",
    "    output_dir=DEV_OUTPUT,\n",
    "    file_namer=extractors.title_to_filename,\n",
    "    serializer=make_serializable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_all_and_write(\n",
    "    corpora,\n",
    "    extract_scenes_and_sequels,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    output_dir=DEV_OUTPUT,\n",
    "    file_namer=extractors.title_to_filename,\n",
    "    serializer=make_serializable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_story_extraction(\n",
    "    story,\n",
    "    model_name,\n",
    "    output_dir,\n",
    "    file_namer\n",
    "    ):\n",
    "    # Determine file path\n",
    "    model_dir = os.path.join(output_dir, \"scene_extraction\", model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    filename = file_namer(story.name) + \"-scenes.json\"\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "\n",
    "    # If the file exists and we're not forcing recomputation, load and return\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "def fetch_or_compute_story_extraction(\n",
    "    story,\n",
    "    extractor,\n",
    "    model_name,\n",
    "    output_dir,\n",
    "    file_namer,\n",
    "    serializer,\n",
    "    force=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch the scene/sequel extraction for a story, loading from disk if available,\n",
    "    or computing and saving it otherwise.\n",
    "\n",
    "    Parameters:\n",
    "        story: A Story object with a .name and .body attribute.\n",
    "        extractor: Function that extracts structured data from story.body.\n",
    "        model_name: Name of the model used for the extraction (e.g., 'gpt-4o').\n",
    "        output_dir: Root directory where extractions are stored.\n",
    "        file_namer: Function to turn a story name into a safe filename.\n",
    "        serializer: Function that removes or transforms non-serializable objects in the result.\n",
    "        force: If True, reprocess the story even if a saved result exists.\n",
    "\n",
    "    Returns:\n",
    "        The structured extraction result (as loaded from JSON).\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine file path\n",
    "    model_dir = os.path.join(output_dir, \"scene_extraction\", model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    filename = file_namer(story.name) + \"-scenes.json\"\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "\n",
    "    # If the file exists and we're not forcing recomputation, load and return\n",
    "    if not force and os.path.exists(filepath):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    # Otherwise compute, serialize, save, and return\n",
    "    result = extractor(story.body, model_name=model_name)\n",
    "    serializable_result = serializer(result)\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(serializable_result, f, indent=2)\n",
    "    return serializable_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_story = corpora.stories[0]\n",
    "example_extraction = fetch_story_extraction(\n",
    "    example_story,\n",
    "    model_name=\"gpt-4o\",\n",
    "    output_dir=DEV_OUTPUT,\n",
    "    file_namer=extractors.title_to_filename,\n",
    ")\n",
    "len(example_extraction['parsed_output']['events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_data = []\n",
    "\n",
    "def collate_story_extractions(corpora, model_name, output_dir):\n",
    "    \"\"\"\n",
    "    Collate the story extractions into a DataFrame.\n",
    "    \"\"\"\n",
    "    extractions = {}\n",
    "    statistics = []\n",
    "    for story in corpora.stories:\n",
    "        extraction = fetch_story_extraction(\n",
    "            story,\n",
    "            model_name=model_name,\n",
    "            output_dir=output_dir,\n",
    "            file_namer=extractors.title_to_filename,\n",
    "        )\n",
    "        extractions[story.name] = extraction\n",
    "        if extraction is None:\n",
    "            parsed = False\n",
    "            events = []\n",
    "            scenes = []\n",
    "            sequels = []\n",
    "            nones = []\n",
    "        else:\n",
    "            parsed = True\n",
    "            events = extraction['parsed_output']['events']\n",
    "            scenes = [e for e in events if e['event_type'] == 'scene']\n",
    "            sequels = [e for e in events if e['event_type'] == 'sequel']\n",
    "            nones = [e for e in events if e['event_type'] == 'none']\n",
    "        statistics.append({\n",
    "            \"story_name\": story.name,\n",
    "            \"characters\": len(story.body),\n",
    "            \"parsed\": parsed,\n",
    "            \"events\": len(events),\n",
    "            \"scenes\": len(scenes),\n",
    "            \"sequels\": len(sequels),\n",
    "            \"nones\": len(nones)\n",
    "        })\n",
    "    return extractions, pd.DataFrame(statistics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_data, gpt_4o_df = collate_story_extractions(\n",
    "    corpora,\n",
    "    model_name=\"gpt-4o\",\n",
    "    output_dir=DEV_OUTPUT,\n",
    ")\n",
    "gpt_4o_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35_data, gpt_35_df = collate_story_extractions(\n",
    "    corpora,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    output_dir=DEV_OUTPUT,\n",
    ")\n",
    "gpt_35_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for story in corpora.stories:\n",
    "    gpt_4o_extraction = gpt_4o_data[story.name]\n",
    "    gpt_35_extraction = gpt_35_data[story.name]\n",
    "    if gpt_4o_extraction is None or gpt_35_extraction is None:\n",
    "        continue\n",
    "    gpt_4o_events = gpt_4o_extraction['parsed_output']['events']\n",
    "    gpt_35_events = gpt_35_extraction['parsed_output']['events']\n",
    "    gpt_4o_scenes = [e for e in gpt_4o_events if e['event_type'] == 'scene']\n",
    "    gpt_35_scenes = [e for e in gpt_35_events if e['event_type'] == 'scene']\n",
    "    gpt_4o_sequels = [e for e in gpt_4o_events if e['event_type'] == 'sequel']\n",
    "    gpt_35_sequels = [e for e in gpt_35_events if e['event_type'] == 'sequel']\n",
    "    gpt_4o_nones = [e for e in gpt_4o_events if e['event_type'] == 'none']\n",
    "    gpt_35_nones = [e for e in gpt_35_events if e['event_type'] == 'none']\n",
    "    print(f\"Story: {story.name}\")\n",
    "    print(f\" - characters: {len(story.body)}\")\n",
    "    print(f\" - events: {len(gpt_4o_events)} (gpt-4o) vs {len(gpt_35_events)} (gpt-3.5-turbo)\")\n",
    "    print(f\" - scenes: {len(gpt_4o_scenes)} (gpt-4o) vs {len(gpt_35_scenes)} (gpt-3.5-turbo)\")\n",
    "    print(f\" - sequels: {len(gpt_4o_sequels)} (gpt-4o) vs {len(gpt_35_sequels)} (gpt-3.5-turbo)\")\n",
    "    print(f\" - nones: {len(gpt_4o_nones)} (gpt-4o) vs {len(gpt_35_nones)} (gpt-3.5-turbo)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LCATS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
