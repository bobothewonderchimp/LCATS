{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reboot of LCATS Story Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import date\n",
    "from typing import List, Dict, Callable, Optional\n",
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third-party modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "from openai import OpenAI\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add imports from within the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to the path so we can import modules from the parent directory.\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from lcats import chunking\n",
    "from lcats import extraction\n",
    "from lcats import stories\n",
    "from lcats import utils\n",
    "from lcats.datasets import torchdata\n",
    "from lcats.gatherers import extractors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the following code is run from lcats/notebooks in VSCode and the data is in lcats/data ...\n",
    "CURRENT_PATH = os.path.abspath(os.curdir)  # This is where the notebook is executing.\n",
    "PROJECT_ROOT = os.path.dirname(CURRENT_PATH)   # This should be the root of the project.\n",
    "DEV_CORPUS = os.path.abspath(os.path.join(PROJECT_ROOT, 'data'))  # Local copy of the data.\n",
    "DEV_OUTPUT = os.path.abspath(os.path.join(PROJECT_ROOT, 'output'))  # Local copy of the data.\n",
    "GIT_CORPUS = os.path.abspath(os.path.join(PROJECT_ROOT, '../corpora'))  # Data in the git repo.\n",
    "OPENIA_API_KEYS_ENV = os.path.abspath(os.path.join(PROJECT_ROOT, '../.secrets/openai_api_keys.env'))  # Local OpenAI API key.\n",
    "\n",
    "def check_path(path, description):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Found {description} at: {path}\")\n",
    "    else:\n",
    "        print(f\"Missing {description} from: {path}\")\n",
    "\n",
    "check_path(DEV_CORPUS, \"DEV_CORPUS\")\n",
    "check_path(DEV_OUTPUT, \"DEV_OUTPUT\")\n",
    "check_path(GIT_CORPUS, \"GIT_CORPUS\")\n",
    "check_path(OPENIA_API_KEYS_ENV, \"OPENIA_API_KEYS_ENV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(OPENIA_API_KEYS_ENV)\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that we can get a client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "print(f\"Loaded OpenAI client: {client} with version: {client._version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the API is working. This week. And that you have credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Write a one-sentence bedtime story about a starship captain visiting a planet.\"\n",
    ")\n",
    "\n",
    "print(f\"Story generated on: {date.today()}:\")\n",
    "utils.pprint(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Story Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the modules to ensure we have the latest code, if doing active development.\n",
    "if False: \n",
    "    from importlib import reload\n",
    "    reload(stories)\n",
    "    reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If run from within a notebook, the corpora root is two paths up from the notebook's location.\n",
    "CORPORA_ROOT = GIT_CORPUS  # Checked-in corpora\n",
    "# CORPORA_ROOT = DEV_CORPUS  # Command line working corpora\n",
    "\n",
    "# Now load the corpora\n",
    "corpora = stories.Corpora(CORPORA_ROOT)\n",
    "\n",
    "print(\"Loaded corpora:\")\n",
    "print(f\" - root: {corpora.corpora_root}\")\n",
    "print(f\" - corpora: {len(corpora.corpora)}\")\n",
    "print(f\" - stories: {len(corpora.stories)}\")\n",
    "print()\n",
    "print(f\"Example story: corpora.stories[0]:\")\n",
    "print(corpora.stories[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_story = corpora.stories[0]\n",
    "print(f\"Story type: {type(example_story)} with a body of {len(example_story.body)} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scene and Sequel Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First-pass Prompts suggested by ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCENE_SEQUEL_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that breaks down stories into structured events.\n",
    "Each event is labeled as \"scene\", \"sequel\", or \"none\" (if it doesn't fit exactly).\n",
    "Follow these definitions:\n",
    "\n",
    "- scene: a segment where a character with a goal attempts to achieve it, leading to success or disaster.\n",
    "- sequel: a segment after a disaster or success, where a character reacts, processes emotions, considers options, and forms a new goal.\n",
    "\n",
    "Your output MUST be valid JSON and only the JSON without any other text or comments.\n",
    "\"\"\"\n",
    "\n",
    "SCENE_SEQUEL_USER_PROMPT_TEMPLATE = \"\"\"\n",
    "I will give you a story in plain text.\n",
    "1. Read the story carefully.\n",
    "2. Identify major events or paragraphs that qualify as scenes or sequels (or 'none' if it doesn't clearly fit).\n",
    "3. For each event, provide:\n",
    "   - event_text: the text snippet or summary\n",
    "   - event_type: 'scene' or 'sequel' or 'none'\n",
    "   - reason: a short explanation of why you classified it that way\n",
    "4. Return a JSON dictionary with one key named \"events\" - the output must be valid JSON and only the JSON.\n",
    "Your output MUST be valid JSON and only the JSON without any other text or comments.\n",
    "\n",
    "STORY:\n",
    "\\\"\\\"\\\"{story_text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "SCENE_SEQUEL_EXTRACTOR = extraction.ExtractionTemplate(\n",
    "    name=\"scene_sequel_extractor\",\n",
    "    system_template=SCENE_SEQUEL_SYSTEM_PROMPT,\n",
    "    user_template=SCENE_SEQUEL_USER_PROMPT_TEMPLATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = SCENE_SEQUEL_EXTRACTOR.build_prompt(example_story.body)\n",
    "example_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scenes_and_sequels(\n",
    "        story_text: str,        \n",
    "        model_name: str = \"gpt-4o\",\n",
    "        temperature: float = 0.2,\n",
    "        template: Callable = SCENE_SEQUEL_EXTRACTOR,\n",
    "    ) -> extraction.ExtractionResult:\n",
    "    \"\"\"\n",
    "    Extract scenes and sequels from a story using the specified model.\n",
    "    \n",
    "    Args:\n",
    "        story_text (str): The text of the story to analyze.\n",
    "        model_name (str): The name of the OpenAI model to use for extraction.\n",
    "    \n",
    "    Returns:\n",
    "        extraction.ExtractionResult: The result of the extraction, including parsed output and any errors.\n",
    "    \"\"\"\n",
    "    return extraction.extract_from_story(\n",
    "        story_text,\n",
    "        template=template,\n",
    "        client=client,\n",
    "        model_name=model_name,\n",
    "        temperature=temperature\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gpt_35_turbo = extract_scenes_and_sequels(example_story.body, model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_gpt_35_turbo.extracted_output), result_gpt_35_turbo.extracted_output[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this cell takes 30-90 seconds and is an expensive GPT 4.0 call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_gpt_4o = extract_scenes_and_sequels(example_story.body, model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_gpt_4o.extracted_output), result_gpt_4o.extracted_output[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Extractions to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_serializable(result, nonserializable_key=\"response\"):\n",
    "    \"\"\"\n",
    "    Remove a non-serializable key from the result dictionary.\n",
    "\n",
    "    Args:\n",
    "        result (dict): The dictionary to clean.\n",
    "        nonserializable_key (str): The key to remove if present.\n",
    "\n",
    "    Returns:\n",
    "        dict: A shallow copy of the dictionary with the specified key removed.\n",
    "    \"\"\"\n",
    "    result = dict(result)  # shallow copy to avoid mutating original\n",
    "    result.pop(nonserializable_key, None)\n",
    "    return result\n",
    "\n",
    "def extract_all_and_write(corpora, extractor, model_name, output_dir, file_namer, serializer):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create a subdirectory for the model\n",
    "    model_dir = os.path.join(output_dir, \"scene_extraction\", model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    for story in corpora.stories:\n",
    "        filename = file_namer(story.name) + \"-scenes.json\"\n",
    "        filepath = os.path.join(model_dir, filename)\n",
    "\n",
    "        # Skip already-processed files\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"Skipping already processed story: {story.name}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"Processing story: {story.name}\")\n",
    "            result = extractor(story.body, model_name=model_name)\n",
    "            serialized_result = serializer(result)\n",
    "            with open(filepath, \"w\") as f:\n",
    "                json.dump(serialized_result, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {story.name}: {e}\")\n",
    "\n",
    "    print(\"Scene extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Scenes for Two Popular Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_all_and_write(\n",
    "    corpora,\n",
    "    extract_scenes_and_sequels,\n",
    "    model_name=\"gpt-4o\",\n",
    "    output_dir=DEV_OUTPUT,\n",
    "    file_namer=extractors.title_to_filename,\n",
    "    serializer=make_serializable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_all_and_write(\n",
    "    corpora,\n",
    "    extract_scenes_and_sequels,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    output_dir=DEV_OUTPUT,\n",
    "    file_namer=extractors.title_to_filename,\n",
    "    serializer=make_serializable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Extraction, Computing if Necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_story_extraction(\n",
    "    story,\n",
    "    model_name,\n",
    "    output_dir,\n",
    "    file_namer\n",
    "    ):\n",
    "    # Determine file path\n",
    "    model_dir = os.path.join(output_dir, \"scene_extraction\", model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    filename = file_namer(story.name) + \"-scenes.json\"\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "\n",
    "    # If the file exists and we're not forcing recomputation, load and return\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return None\n",
    "\n",
    "def fetch_or_compute_story_extraction(\n",
    "    story,\n",
    "    extractor,\n",
    "    model_name,\n",
    "    output_dir,\n",
    "    file_namer,\n",
    "    serializer,\n",
    "    force=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch the scene/sequel extraction for a story, loading from disk if available,\n",
    "    or computing and saving it otherwise.\n",
    "\n",
    "    Parameters:\n",
    "        story: A Story object with a .name and .body attribute.\n",
    "        extractor: Function that extracts structured data from story.body.\n",
    "        model_name: Name of the model used for the extraction (e.g., 'gpt-4o').\n",
    "        output_dir: Root directory where extractions are stored.\n",
    "        file_namer: Function to turn a story name into a safe filename.\n",
    "        serializer: Function that removes or transforms non-serializable objects in the result.\n",
    "        force: If True, reprocess the story even if a saved result exists.\n",
    "\n",
    "    Returns:\n",
    "        The structured extraction result (as loaded from JSON).\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine file path\n",
    "    model_dir = os.path.join(output_dir, \"scene_extraction\", model_name)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    filename = file_namer(story.name) + \"-scenes.json\"\n",
    "    filepath = os.path.join(model_dir, filename)\n",
    "\n",
    "    # If the file exists and we're not forcing recomputation, load and return\n",
    "    if not force and os.path.exists(filepath):\n",
    "        with open(filepath, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    # Otherwise compute, serialize, save, and return\n",
    "    result = extractor(story.body, model_name=model_name)\n",
    "    serializable_result = serializer(result)\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(serializable_result, f, indent=2)\n",
    "    return serializable_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_story = corpora.stories[0]\n",
    "example_extraction = fetch_story_extraction(\n",
    "    example_story,\n",
    "    model_name=\"gpt-4o\",\n",
    "    output_dir=DEV_OUTPUT,\n",
    "    file_namer=extractors.title_to_filename,\n",
    ")\n",
    "len(example_extraction['parsed_output']['events'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the First Pass Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def collate_story_extractions(corpora, model_name, output_dir):\n",
    "    \"\"\"\n",
    "    Collate the story extractions into a DataFrame.\n",
    "    \"\"\"\n",
    "    extractions = {}\n",
    "    statistics = []\n",
    "    for story in corpora.stories:\n",
    "        # Get metadata about the story\n",
    "        story_text = story.body\n",
    "        characters = len(story_text)\n",
    "        tokens = count_tokens(story_text, model_name)\n",
    "\n",
    "        # Get the extraction for the story\n",
    "        extraction = fetch_story_extraction(\n",
    "            story,\n",
    "            model_name=model_name,\n",
    "            output_dir=output_dir,\n",
    "            file_namer=extractors.title_to_filename,\n",
    "        )\n",
    "        extractions[story.name] = extraction\n",
    "        if extraction is None:\n",
    "            # print(f\"Missing extraction for story: {story.name}\")\n",
    "            parsed = False\n",
    "            events = []\n",
    "            scenes = []\n",
    "            sequels = []\n",
    "            nones = []\n",
    "        else:\n",
    "            # print(f\"Processing extraction for story: {story.name}\")\n",
    "            parsed = True\n",
    "            events = extraction['parsed_output']['events']\n",
    "            scenes = [e for e in events if e['event_type'] == 'scene']\n",
    "            sequels = [e for e in events if e['event_type'] == 'sequel']\n",
    "            nones = [e for e in events if e['event_type'] == 'none']\n",
    "\n",
    "        # Append the statistics for this story\n",
    "        statistics.append({\n",
    "            \"story_name\": story.name,\n",
    "            \"characters\": characters,\n",
    "            \"tokens\": tokens,\n",
    "            \"parsed\": int(parsed),\n",
    "            \"events\": len(events),\n",
    "            \"scenes\": len(scenes),\n",
    "            \"sequels\": len(sequels),\n",
    "            \"nones\": len(nones)\n",
    "        })\n",
    "    return extractions, pd.DataFrame(statistics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_data, gpt_4o_df = collate_story_extractions(\n",
    "    corpora,\n",
    "    model_name=\"gpt-4o\",\n",
    "    output_dir=DEV_OUTPUT,\n",
    ")\n",
    "print(gpt_4o_df.describe())\n",
    "gpt_4o_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35_data, gpt_35_df = collate_story_extractions(\n",
    "    corpora,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    output_dir=DEV_OUTPUT,\n",
    ")\n",
    "print(gpt_35_df.describe())\n",
    "gpt_35_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_df[gpt_4o_df[\"story_name\"] == \"Sherlock Holmes - The Adventure of the Engineer's Thumb\"][\"tokens\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = []\n",
    "for story in corpora.stories:\n",
    "    gpt_4o_extraction = gpt_4o_data[story.name]\n",
    "    gpt_35_extraction = gpt_35_data[story.name]\n",
    "    if gpt_4o_extraction is None or gpt_35_extraction is None:\n",
    "        continue\n",
    "    gpt_4o_tokens = gpt_4o_df[gpt_4o_df[\"story_name\"] == story.name]['tokens'].values[0]\n",
    "    gpt_35_tokens = gpt_35_df[gpt_35_df[\"story_name\"] == story.name]['tokens'].values[0]\n",
    "    gpt_4o_events = gpt_4o_extraction['parsed_output']['events']\n",
    "    gpt_35_events = gpt_35_extraction['parsed_output']['events']\n",
    "    gpt_4o_scenes = [e for e in gpt_4o_events if e['event_type'] == 'scene']\n",
    "    gpt_35_scenes = [e for e in gpt_35_events if e['event_type'] == 'scene']\n",
    "    gpt_4o_sequels = [e for e in gpt_4o_events if e['event_type'] == 'sequel']\n",
    "    gpt_35_sequels = [e for e in gpt_35_events if e['event_type'] == 'sequel']\n",
    "    gpt_4o_nones = [e for e in gpt_4o_events if e['event_type'] == 'none']\n",
    "    gpt_35_nones = [e for e in gpt_35_events if e['event_type'] == 'none']\n",
    "    print(f\"Story: {story.name}\")\n",
    "    print(f\" - characters: {len(story.body)}\")\n",
    "    print(f\" - tokens: {gpt_4o_tokens} (gpt-4o) vs {gpt_35_tokens} (gpt-3.5-turbo)\")\n",
    "    print(f\" - events: {len(gpt_4o_events)} (gpt-4o) vs {len(gpt_35_events)} (gpt-3.5-turbo)\")\n",
    "    print(f\" - scenes: {len(gpt_4o_scenes)} (gpt-4o) vs {len(gpt_35_scenes)} (gpt-3.5-turbo)\")\n",
    "    print(f\" - sequels: {len(gpt_4o_sequels)} (gpt-4o) vs {len(gpt_35_sequels)} (gpt-3.5-turbo)\")\n",
    "    print(f\" - nones: {len(gpt_4o_nones)} (gpt-4o) vs {len(gpt_35_nones)} (gpt-3.5-turbo)\")\n",
    "    comparisons.append({\n",
    "        \"story_name\": story.name,\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"tokens\": gpt_4o_tokens,\n",
    "        \"events\": len(gpt_4o_events),\n",
    "        \"scenes\": len(gpt_4o_scenes),\n",
    "        \"sequels\": len(gpt_4o_sequels),\n",
    "        \"nones\": len(gpt_4o_nones)\n",
    "    })\n",
    "    comparisons.append({\n",
    "        \"story_name\": story.name,\n",
    "        \"model\": \"gpt-3.5-turbo\",\n",
    "        \"tokens\": gpt_35_tokens,\n",
    "        \"events\": len(gpt_35_events),\n",
    "        \"scenes\": len(gpt_35_scenes),\n",
    "        \"sequels\": len(gpt_35_sequels),\n",
    "        \"nones\": len(gpt_35_nones)\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(comparison_df.columns)\n",
    "print(comparison_df.describe())\n",
    "comparison_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_4o = gpt_4o_df[gpt_35_df[\"parsed\"] == 1]\n",
    "common_35 = gpt_35_df[gpt_35_df[\"parsed\"] == 1]\n",
    "print(\"Common stories with valid extractions:\")\n",
    "print(f\" - gpt-4o: {len(common_4o)}\")\n",
    "print(common_4o.describe())\n",
    "print(f\" - gpt-3.5-turbo: {len(common_35)}\")\n",
    "print(common_35.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_data[example_story.name]['parsed_output']['events'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35_data[example_story.name]['parsed_output']['events'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_columns_vs(df, x_col, y_col, hue_col=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=df, x=x_col, y=y_col, hue=hue_col)\n",
    "    plt.xlabel(x_col)\n",
    "    plt.ylabel(y_col)\n",
    "    plt.title(f\"{y_col} vs {x_col}\")\n",
    "    plt.legend(title=hue_col)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_columns_vs(comparison_df, 'tokens', 'events', hue_col='model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_columns_vs(comparison_df, 'tokens', 'scenes', hue_col='model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_columns_vs(comparison_df, 'tokens', 'nones', hue_col='model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk-Based Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max tokens divided by min scenes gives us a rough estimate of how many tokens are needed per scene.\n",
    "estimated_tokens_per_scene = float(comparison_df['tokens'].max() / comparison_df['scenes'].min())\n",
    "estimated_tokens_per_scene\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_story = corpora.stories[0]\n",
    "story_text = example_story.body\n",
    "chunks = chunking.chunk_story(story_text, max_tokens=6000, overlap_tokens=0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "print(f\"Chunked story into {len(chunks)} parts.\")\n",
    "for index, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {index} ({len(chunk.text)} chars):\")\n",
    "    print(chunk.text[:100] + \"...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking with overlap is likely smarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_story = corpora.stories[0]\n",
    "example_text = example_story.body\n",
    "\n",
    "example_chunks = chunking.chunk_story(example_text, max_tokens=6000, overlap_tokens=200, model_name=\"gpt-3.5-turbo\")\n",
    "chunking.display_chunks(example_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Scenes - LASTREVIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SceneSpan:\n",
    "    chunk_index: int\n",
    "    relative_start: int\n",
    "    relative_end: int\n",
    "    scene_type: str\n",
    "    text: str\n",
    "    offset: int  # character offset in original story\n",
    "\n",
    "    def absolute_start(self) -> int:\n",
    "        return self.offset + self.relative_start\n",
    "\n",
    "    def absolute_end(self) -> int:\n",
    "        return self.offset + self.relative_end\n",
    "\n",
    "SCENE_SEQUEL_SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful assistant that breaks down stories into structured narrative events.\n",
    "\n",
    "Each event must be labeled as:\n",
    "- \"scene\": a segment where a character pursues a goal, leading to success or failure.\n",
    "- \"sequel\": a segment where a character reacts to the outcome, reflects, and formulates a new plan.\n",
    "- \"none\": a segment that does not clearly fit into either category (e.g., exposition or transition).\n",
    "\n",
    "You MUST output a list of events as a JSON dictionary with one key: \"events\".\n",
    "\n",
    "Each event must include:\n",
    "- \"type\": one of \"scene\", \"sequel\", or \"none\"\n",
    "- \"reason\": a brief explanation of why you classified it this way\n",
    "- \"text\": the exact event text, copied from the story\n",
    "- \"start_char\": the starting character offset of the event in the original story\n",
    "- \"end_char\": the ending character offset (exclusive)\n",
    "\n",
    "Do NOT include any additional explanation or formatting.\n",
    "Only return valid JSON.\n",
    "\"\"\"\n",
    "\n",
    "SCENE_SEQUEL_USER_PROMPT_TEMPLATE = \"\"\"\n",
    "I will give you a story in plain text.\n",
    "\n",
    "Please identify contiguous narrative segments that represent:\n",
    "- scenes (goal-driven attempts)\n",
    "- sequels (reflections after outcomes)\n",
    "- or neither (\"none\" for other material)\n",
    "\n",
    "For each segment, return a dictionary with:\n",
    "- \"type\": \"scene\", \"sequel\", or \"none\"\n",
    "- \"reason\": a brief explanation of why you classified it this way\n",
    "- \"text\": the exact span of text from the story\n",
    "- \"start_char\": starting character index of this span in the story\n",
    "- \"end_char\": ending character index (exclusive)\n",
    "\n",
    "Return a single JSON object with a key named \"events\" and a list of these entries.\n",
    "\n",
    "The output MUST be valid JSON and include ONLY the JSON, no explanation or comments.\n",
    "\n",
    "STORY:\n",
    "\\\"\\\"\\\"{story_text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "def build_scene_sequel_prompt(story_text: str) -> list:\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SCENE_SEQUEL_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": SCENE_SEQUEL_USER_PROMPT_TEMPLATE.format(story_text=story_text)}\n",
    "    ]\n",
    "\n",
    "\n",
    "def extract_scenes_and_sequels(story_text: str, model_name=\"gpt-3.5-turbo\") -> Dict:\n",
    "    messages = build_scene_sequel_prompt(story_text)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "\n",
    "    raw_output = response.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        parsed_output = utils.extract_json(raw_output)\n",
    "        parsing_error = None\n",
    "    except json.JSONDecodeError as exc:\n",
    "        parsed_output = None\n",
    "        parsing_error = str(exc)\n",
    "\n",
    "    scene_spans: List[SceneSpan] = []\n",
    "    extraction_error = None\n",
    "\n",
    "    if isinstance(parsed_output, dict) and \"events\" in parsed_output:\n",
    "        for i, event in enumerate(parsed_output[\"events\"]):\n",
    "            scene_text = event.get(\"text\", \"\").strip()\n",
    "            scene_type = event.get(\"type\", \"scene\")\n",
    "\n",
    "            # Match scene_text in story_text to find offsets\n",
    "            match = re.search(re.escape(scene_text), story_text)\n",
    "            if match:\n",
    "                start_char = match.start()\n",
    "                end_char = match.end()\n",
    "            else:\n",
    "                start_char = None\n",
    "                end_char = None\n",
    "\n",
    "            scene_spans.append(SceneSpan(\n",
    "                chunk_index=0,  # Actual index will be set later by tail extraction logic\n",
    "                relative_start=0,  # Will be calculated by offset adjustment later\n",
    "                relative_end=0,\n",
    "                scene_type=scene_type,\n",
    "                text=scene_text,\n",
    "                offset=start_char if start_char is not None else 0  # fallback\n",
    "            ))\n",
    "\n",
    "            # Patch relative positions if match succeeded\n",
    "            if match:\n",
    "                scene_spans[-1].relative_start = 0  # initially 0, will be corrected in stitching\n",
    "                scene_spans[-1].relative_end = end_char - start_char\n",
    "\n",
    "    else:\n",
    "        scene_spans = []\n",
    "        extraction_error = \"Expected 'events' key in JSON response.\"\n",
    "\n",
    "    return {\n",
    "        \"story_text\": story_text,\n",
    "        \"model_name\": model_name,\n",
    "        \"messages\": messages,\n",
    "        \"response\": response,\n",
    "        \"raw_output\": raw_output,\n",
    "        \"parsed_output\": parsed_output,\n",
    "        \"extracted_output\": scene_spans,\n",
    "        \"parsing_error\": parsing_error,\n",
    "        \"extraction_error\": extraction_error,\n",
    "    }\n",
    "\n",
    "\n",
    "def span_is_complete(span: Dict) -> bool:\n",
    "    \"\"\"\n",
    "    Simple heuristic to determine if a scene span is complete.\n",
    "    \"\"\"\n",
    "    text = span.get(\"text\", \"\").strip()\n",
    "    return text.endswith(\".\") and not text.endswith(\"...\") and len(text) > 20\n",
    "\n",
    "def get_span_from_raw(\n",
    "    raw: Dict,\n",
    "    chunk_index: int,\n",
    "    chunk_input_text: str,\n",
    "    carryover_len: int,\n",
    "    chunk_offset: int\n",
    ") -> Optional[SceneSpan]:\n",
    "    \"\"\"\n",
    "    Convert a raw dict (LLM output) into a SceneSpan object.\n",
    "    Uses fallback matching if start_char and end_char are missing.\n",
    "    \"\"\"\n",
    "    scene_text = raw.get(\"text\", \"\").strip()\n",
    "    scene_type = raw.get(\"type\", \"scene\")\n",
    "\n",
    "    start_char = raw.get(\"start_char\")\n",
    "    end_char = raw.get(\"end_char\")\n",
    "\n",
    "    if start_char is None or end_char is None:\n",
    "        match = re.search(re.escape(scene_text), chunk_input_text)\n",
    "        if not match:\n",
    "            return None\n",
    "        start_char = match.start()\n",
    "        end_char = match.end()\n",
    "\n",
    "    adjusted_start = max(0, start_char - carryover_len)\n",
    "    adjusted_end = max(0, end_char - carryover_len)\n",
    "    print(f\"Processing span: {scene_text} (start: {start_char}, end: {end_char}) \"\n",
    "          f\"-> adjusted start: {adjusted_start}, adjusted end: {adjusted_end}\")\n",
    "    if adjusted_end <= 0:\n",
    "        print(f\"Skipping span entirely in carryover: {scene_text}\")\n",
    "        return None  # entirely in carryover, skip\n",
    "\n",
    "    return SceneSpan(\n",
    "        chunk_index=chunk_index,\n",
    "        relative_start=adjusted_start,\n",
    "        relative_end=adjusted_end,\n",
    "        scene_type=scene_type,\n",
    "        text=chunk_input_text[start_char:end_char],\n",
    "        offset=chunk_offset\n",
    "    )\n",
    "\n",
    "def extract_carryover_text(\n",
    "    raw_span: Dict,\n",
    "    chunk_input_text: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Extract tail of the final scene span for prepending to the next chunk.\n",
    "    Falls back on re-matching if no indices are given.\n",
    "    \"\"\"\n",
    "    last_text = raw_span.get(\"text\", \"\")\n",
    "    start = raw_span.get(\"start_char\")\n",
    "    end = raw_span.get(\"end_char\")\n",
    "\n",
    "    if start is not None and end is not None:\n",
    "        return chunk_input_text[start:end]\n",
    "\n",
    "    match = re.search(re.escape(last_text), chunk_input_text)\n",
    "    return chunk_input_text[match.start():match.end()] if match else \"\"\n",
    "\n",
    "\n",
    "def extract_scene_spans_with_tail(\n",
    "    chunks: List[Chunk],\n",
    "    model_name: str,\n",
    "    extract_fn: Callable[[str, str], Dict]\n",
    ") -> List[SceneSpan]:\n",
    "    stitched_spans: List[SceneSpan] = []\n",
    "    carryover_text = \"\"\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Processing chunk {i + 1}/{len(chunks)}: {len(chunks[i].text)} chars\")\n",
    "        chunk_input_text = carryover_text + chunk.text\n",
    "        carryover_len = len(carryover_text)\n",
    "\n",
    "        # Call the model extractor\n",
    "        result = extract_fn(chunk_input_text, model_name=model_name)\n",
    "        raw_spans = result.get(\"extracted_output\", [])\n",
    "        print(f\"Extracted {len(raw_spans)} spans from chunk {i + 1}\")\n",
    "\n",
    "        # Try to convert all spans to SceneSpan\n",
    "        for raw in raw_spans:\n",
    "            span = get_span_from_raw(\n",
    "                raw,\n",
    "                chunk_index=i,\n",
    "                chunk_input_text=chunk_input_text,\n",
    "                carryover_len=carryover_len,\n",
    "                chunk_offset=chunk.start_char\n",
    "            )\n",
    "            if span:\n",
    "                stitched_spans.append(span)\n",
    "\n",
    "        # Setup carryover if final span is incomplete\n",
    "        if raw_spans and not span_is_complete(raw_spans[-1]):\n",
    "            carryover_text = extract_carryover_text(raw_spans[-1], chunk_input_text)\n",
    "        else:\n",
    "            carryover_text = \"\"\n",
    "\n",
    "    return stitched_spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_chunk = example_chunks[0]\n",
    "example_text = example_chunk.text\n",
    "example_prompt = build_scene_sequel_prompt(example_text)\n",
    "example_result = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=example_prompt,\n",
    "    temperature=0.2,\n",
    ")\n",
    "example_raw_output = example_result.choices[0].message.content\n",
    "example_parsed_output = utils.extract_json(example_raw_output)\n",
    "example_spans = example_parsed_output[\"events\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(example_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_spans[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, length=72):\n",
    "    \"\"\"\n",
    "    Summarize the text to a specified length.\n",
    "    \"\"\"\n",
    "    if len(text) <= length:\n",
    "        return text\n",
    "    prefix = length // 2\n",
    "    return text[:prefix] + \"...\" + text[-(length - prefix):]\n",
    "\n",
    "\n",
    "for i, span in enumerate(example_spans):\n",
    "    # print(span)\n",
    "    print(f\"Span {i} type: {span.get('type')}, text: {summarize(span.get('text', ''))}\")\n",
    "    start_char = span.get(\"start_char\")\n",
    "    end_char = span.get(\"end_char\")\n",
    "    if start_char is not None and end_char is not None:\n",
    "        span_text = example_text[start_char:end_char]\n",
    "        print(f\" - start: {start_char}, end: {end_char}, text: {summarize(span_text)}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_chunk = example_chunks[0]\n",
    "extraction = extract_scenes_and_sequels(\n",
    "    example_chunk.text, model_name=\"gpt-3.5-turbo\")\n",
    "extraction['extracted_output'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_scene_spans_with_tail(\n",
    "    example_chunks,\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    extract_fn=extract_scenes_and_sequels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks), chunks[0][:100], chunks[-1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {index} ({len(chunk)} chars):\")\n",
    "    print(chunk[:100] + \"...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LCATS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
