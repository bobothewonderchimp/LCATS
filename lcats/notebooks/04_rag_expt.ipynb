{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import datasets\n",
    "import langchain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import faiss\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "import semantic_kernel as sk\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "def key_width(d):\n",
    "    return max(len(str(k)) for k in d.keys())\n",
    "\n",
    "def print_dict(d):\n",
    "    width = key_width(d) + 1\n",
    "    for k, v in d.items():\n",
    "        # It's a quiet English format string, and you are a horrible goose.\n",
    "        print(f'{{k:{width}}}: {{v}}'.format(k=k, v=v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROCStories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROCStories corpus might be a good one to just test our ability to load a dataset and use it with various language chain tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Load the ROCStories dataset\n",
    "dataset = datasets.load_dataset('Ximing/ROCStories')\n",
    "\n",
    "# Explore the dataset\n",
    "element = dataset['train'][0]\n",
    "print_dict(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCATS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define paths to our corpora and environment files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# If the following code is run from lcats/notebooks in VSCode and the data is in lcats/data ...\n",
    "CURRENT_PATH = os.path.abspath(os.curdir)  # This is where the notebook is executing.\n",
    "PROJECT_ROOT = os.path.dirname(CURRENT_PATH)   # This should be the root of the project.\n",
    "DEV_CORPUS = os.path.abspath(os.path.join(PROJECT_ROOT, 'data'))  # Local copy of the data.\n",
    "GIT_CORPUS = os.path.abspath(os.path.join(PROJECT_ROOT, '../corpora'))  # Data in the git repo.\n",
    "OPENIA_API_KEYS_ENV = os.path.abspath(os.path.join(PROJECT_ROOT, '../.secrets/openai_api_keys.env'))  # Local OpenAI API key.\n",
    "\n",
    "DEV_CORPUS, GIT_CORPUS, OPENIA_API_KEYS_ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(DEV_CORPUS), f\"DEV_CORPUS does not exist: {DEV_CORPUS}\"\n",
    "assert os.path.exists(GIT_CORPUS), f\"GIT_CORPUS does not exist: {GIT_CORPUS}\"\n",
    "assert os.path.exists(OPENIA_API_KEYS_ENV), f\"API_ENV does not exist: {OPENIA_API_KEYS_ENV}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv(OPENIA_API_KEYS_ENV)\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "print(OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, get copies of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "def load_corpus(data_dir):\n",
    "    corpus = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                with open(os.path.join(root, file)) as f:\n",
    "                    data = json.load(f)\n",
    "                    corpus.append({\n",
    "                        'name': data['name'],\n",
    "                        'body': data['body'],\n",
    "                        'metadata': data['metadata'],\n",
    "                    })\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have 20-30 files if all goes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "lcats_corpus = load_corpus(DEV_CORPUS)\n",
    "len(lcats_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(story_text, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in the given story text for a specific OpenAI model.\n",
    "\n",
    "    Parameters:\n",
    "        story_text (str): The text to tokenize.\n",
    "        model (str): The OpenAI model to use for tokenization. Default is \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "        int: The number of tokens in the story text.\n",
    "    \"\"\"\n",
    "    # Get the tokenizer for the specified model\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = encoding.encode(story_text)\n",
    "    \n",
    "    # Return the number of tokens\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcats_analysis = []\n",
    "for story in lcats_corpus:\n",
    "    story_name = story['name']\n",
    "    story_text = story['body']\n",
    "    story_len = len(story_text)\n",
    "    story_tokens = count_tokens(story_text)\n",
    "    readable_by_gpt_3_5 = story_tokens < 4096\n",
    "    readable_by_gpt_4o = story_tokens < 32768\n",
    "    lcats_analysis.append({\n",
    "        'name': story_name,\n",
    "        'length': story_len,\n",
    "        'tokens': story_tokens,\n",
    "        'readable_by_gpt_3_5': readable_by_gpt_3_5,\n",
    "        'readable_by_gpt_4o': readable_by_gpt_4o,\n",
    "    })\n",
    "lcats_analysis = pd.DataFrame(lcats_analysis)\n",
    "lcats_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "def get_entry_text(entry):\n",
    "    return f\"{entry['name']}\\n{entry['body']}\"\n",
    "\n",
    "def chunk_text_for_embeddings(text):\n",
    "    chunks = text.split(\"\\n\\n\")\n",
    "    return chunks\n",
    "\n",
    "def get_embeddings_for_text(text):\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-ada-002\"  # Specify the embedding model\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding: {e}\")\n",
    "        return None\n",
    "    \n",
    "def get_embeddings_for_corpus(corpus):\n",
    "    print(\"-\" * 72)\n",
    "    print(f\"Generating embeddings for {len(corpus)} corpus entries.\")\n",
    "    print(\"-\" * 72)\n",
    "    corpus_with_embeddings = []\n",
    "    for entry in corpus:\n",
    "        print(f\"Generating embeddings for {entry['name']}:\")\n",
    "        text = get_entry_text(entry)\n",
    "        chunks = chunk_text_for_embeddings(text)\n",
    "        print(f\"  {len(chunks)} chunks found.\")\n",
    "        for chunk in tqdm(chunks):\n",
    "            embedding = get_embeddings_for_text(chunk)\n",
    "            corpus_with_embeddings.append({\n",
    "                'text': chunk,\n",
    "                'embedding': embedding,\n",
    "                'metadata': entry['metadata']\n",
    "            })\n",
    "        print()\n",
    "    return corpus_with_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make it possible to save and load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings_json(data, filepath):\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "def load_embeddings_json(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for each chunk\n",
    "if False:\n",
    "    corpus_with_embeddings = get_embeddings_for_corpus(lcats_corpus)\n",
    "    save_embeddings_json(corpus_with_embeddings, 'output/lcats_corpus_embeddings.json')\n",
    "else:\n",
    "    corpus_with_embeddings = load_embeddings_json('output/lcats_corpus_embeddings.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the loaded corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(corpus_with_embeddings), \n",
    " corpus_with_embeddings[0].keys(), \n",
    " len(corpus_with_embeddings[0]['embedding']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunk(chunk):\n",
    "    story_name = chunk['metadata']['name']\n",
    "    story_author = chunk['metadata']['author']\n",
    "    embedding = chunk['embedding']\n",
    "    embedding_len = len(embedding)\n",
    "    text = chunk['text']\n",
    "    text_len = len(text)\n",
    "    text_tokens = count_tokens(text)\n",
    "\n",
    "    print(f\"Chunk from Story: '{story_name}' by {story_author}\")\n",
    "    print(f\" - Embedding ({embedding_len} elements): {embedding[:3] + ['...']}\")\n",
    "    print(f\" - Snippet ({text_len} characters, {text_tokens} tokens): '{text.strip()}'\")\n",
    "\n",
    "summarize_chunk(random.choice(corpus_with_embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index\n",
    "dimension = len(corpus_with_embeddings[0]['embedding'])\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to the index\n",
    "embeddings = np.array([item['embedding'] for item in corpus_with_embeddings])\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve from the vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_for_query(query, top_n=5):\n",
    "    query_embedding = get_embeddings_for_text(query)\n",
    "    _, indices = index.search(np.array([query_embedding]), top_n)\n",
    "    return [corpus_with_embeddings[i] for i in indices[0]]\n",
    "\n",
    "for chunk in get_chunks_for_query(\"The cat sat on the mat.\"):\n",
    "    summarize_chunk(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_from_chunks(chunks):\n",
    "    return \"\\n\".join([chunk['text'] for chunk in chunks])\n",
    "\n",
    "def generate_prompt_from_query_and_context(query, context):\n",
    "    return f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "def elaborate_query_with_context(query):\n",
    "    chunks = get_chunks_for_query(query)\n",
    "    context = generate_context_from_chunks(chunks)\n",
    "    return generate_prompt_from_query_and_context(query, context)\n",
    "    \n",
    "elaborate_query_with_context(\"What did that cat do?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_completions(prompt, max_tokens=100):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def retrieve_and_generate(query):\n",
    "    elaborated_query = elaborate_query_with_context(query)\n",
    "    return generate_completions(elaborated_query)\n",
    "\n",
    "retrieve_and_generate(\"Who is Sherlock's friend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_and_generate(\"What year was The Adventure of the Engineer's Thumb written?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_completions(\"Who is Sherlock's friend?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEXTUP: STORY ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LCATS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
